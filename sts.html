<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>sts</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="./index.html">Home</a></p>
<p>Akshay Gulabrao 11 July 2025</p>
<h1
id="leveraging-speech-to-text-models-for-financial-insights">Leveraging
Speech-to-Text Models for Financial Insights</h1>
<p>Experts in their field routinely publish podcasts on YouTube.
Speech-to-Text models may prove to be a useful tool to efficiently bring
their insights to the stock market. In this paper, I introduce a
pipeline that can be used to bring financial insights to the stock
market in an efficient manner, using Martin Shkreli, a biotech expert as
an example.</p>
<hr />
<h2 id="leveraging-youtube-for-financial-insights">Leveraging YouTube
for Financial Insights</h2>
<p>I’m currently using <strong>yt-dlp</strong>,
<strong>Whisper</strong>, and <strong>text-embedding-3-small</strong> to
listen to everything Martin Shkreli says. The 3-small embeddings go into
a <strong>SQLite3</strong> database that I query before feeding the text
to an LLM.</p>
<ul>
<li><strong>yt-dlp</strong><br />
</li>
<li><strong>Whisper</strong><br />
</li>
<li><strong>text-embedding-3-small</strong><br />
</li>
<li><strong>llm CLI tool</strong> (by Simon Willison)<br />
</li>
<li><strong>Runpod.io</strong></li>
</ul>
<hr />
<h2 id="components">Components</h2>
<h3 id="yt-dlp">yt-dlp</h3>
<p><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp</a> downloads
YouTube videos by simulating a browser with all relevant cookies,
rotating proxies to avoid IP bans, and making the exact same API calls
that would be used to watch a YouTube video online.<br />
⚠️ This may not be a stable solution for very long; YouTube could start
requiring a proof-of-origin tag.</p>
<h3 id="whisper">Whisper</h3>
<p>OpenAI’s open-source speech-to-text model. I use it to convert the
raw audio from each YouTube video into a <code>.vtt</code>
transcript.<br />
Future work: I may switch to a hosted API to reduce operational
overhead.</p>
<h3 id="runpod.io">Runpod.io</h3>
<p>Currently used for two things:<br />
1. Downloading the video.<br />
2. Running Whisper.</p>
<p>I don’t think this is a good long-term choice; I want my research to
focus on <strong>building the dataset</strong>, not on infrastructure
innovation.</p>
<h3 id="text-embedding-3-small">text-embedding-3-small</h3>
<p>OpenAI’s official embedding model. This is my biggest cost
driver—roughly <strong>$0.01 per day</strong> at current volume. I
should migrate away from it as soon as possible.</p>
<h3 id="llm-cli-tool">llm CLI tool</h3>
<p>A command-line tool created by <a
href="https://github.com/simonw/llm">Simon Willison</a>. Excellent
documentation and dead-simple to use.</p>
<hr />
<h2 id="data">Data</h2>
<p>My main contribution should be <strong>the dataset itself</strong>
and <strong>Whisper fine-tuning methods</strong>. Avoiding a transcript
API might still be worthwhile to maintain full control over the
pipeline.</p>
</body>
</html>
