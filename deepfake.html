<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>deepfake</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="./index.html">Home</a></p>
<p>Akshay Gulabrao 13 September 2025</p>
<h1 id="introduction-to-the-world-of-deepfakes">introduction to the
world of deepfakes</h1>
<p><em>This is not a serious introduction to deepfakes. This is my
attempt to become familiar with the field.</em></p>
<p>Face synthesis, aka <em>deepfakes</em>, uses deep learning to convert
a video of 1 person talking to look like another person talking. Because
of it’s potential for misuse, it’s not a very well published idea in the
deep learning community.</p>
<p>Pei et al. 2024 <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>covers a benchmark and survey of
deepfake generation and detection. Conceptually, given input frames,
audio, or text, we want to convert it to output of output frames, audio,
and text. There are 4 subtasks in deepfakes: Face swapping, face
reenactment, talking face generation, and facial attribute editing. I’m
mainly interested in talking face generation, so I will be focusing on
that. There are 2 frontier methods for this task involve Diffusion based
and NeRFS.</p>
<p>Xu et al. proposes direct learning from a single image and audio <a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>. They use the VoxCeleb2 dataset <a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>. I will attempt to reproduce their
experiments with</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://arxiv.org/pdf/2403.17881">Pei et
al. Deepfake Generation and Detection: A Benchmark and Survey.
2024</a><a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://arxiv.org/pdf/2404.10667">Xu et
al. VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time.
2024</a><a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a
href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html">J.S.
Chung. The VoxCeleb2 Dataset</a><a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
