<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Akshay Gulabrao" />
  <title>Voice Conversion</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Voice Conversion</h1>
<p class="author">Akshay Gulabrao</p>
<p class="date">2025-September-16</p>
</header>
<p><a href="./index.html">Home</a></p>
<p>This is not a serious introduction to voice synthesis. This is my
attempt to become familiar with the field.</p>
<p>Voice synthesis refers to the deep learning task of converting one
voice speaking a specific utterance to a different voice. It is
significantly easier than face generation because the data has far lower
dimensionality. Azzuni et al. published a comprehensive survey of voice
cloning, so I will begin there <span class="citation"
data-cites="azzuni2025voicecloning">(<a
href="#ref-azzuni2025voicecloning" role="doc-biblioref">Azzuni
2025</a>)</span>.</p>
<p>Reading the introduction to their paper, the papers they cite mainly
involve TTS speech synthesis; however, I’m mainly interested in
speech-to-speech synthesis. An arXiv search of speech synthesis yielded
the interesting paper by Quamer et al. <span class="citation"
data-cites="quamer2025darkstream">(<a href="#ref-quamer2025darkstream"
role="doc-biblioref">Quamer 2025</a>)</span>. The author uses the
CommonVoice dataset <span class="citation"
data-cites="ardila2020commonvoice">(<a href="#ref-ardila2020commonvoice"
role="doc-biblioref">Ardila et al. 2020</a>)</span>.</p>
<p>Another interesting paper that I found by Luo et al. innovates on a
codec framework with a subspace orthogonal projection module that splits
the input into two subspaces—one correlates to speech and another
correlates to the background <span class="citation"
data-cites="luo2025decodec">(<a href="#ref-luo2025decodec"
role="doc-biblioref">Luo et al. 2025</a>)</span>. See also <span
class="citation"
data-cites="yao2025pureformervc babyvedat_vctk kameoka2025latentvoicegrad">(<a
href="#ref-yao2025pureformervc" role="doc-biblioref">Yao et al.
2025</a>; <a href="#ref-babyvedat_vctk" role="doc-biblioref">babyvedat
2024</a>; <a href="#ref-kameoka2025latentvoicegrad"
role="doc-biblioref">Kameoka et al. 2025</a>)</span>.</p>
<p>The VCTK dataset is extremely popular for voice synthesis research.
It is a 10GB dataset which contains speech data for 100+ native English
speakers with various accents <span class="citation"
data-cites="yamagishi2019cstr">(<a href="#ref-yamagishi2019cstr"
role="doc-biblioref">Yamagishi, Veaux, and MacDonald
2019</a>)</span>.</p>
<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-ardila2020commonvoice" class="csl-entry" role="listitem">
Ardila, R., M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R.
Morais, L. Saunders, F. M. Tyers, and G. Weber. 2020. <span>“Common
Voice: A Massively-Multilingual Speech Corpus.”</span> In
<em>Proceedings of the 12th Conference on Language Resources and
Evaluation (LREC 2020)</em>, 4211–15. <a
href="https://arxiv.org/pdf/1912.06670">https://arxiv.org/pdf/1912.06670</a>.
</div>
<div id="ref-azzuni2025voicecloning" class="csl-entry" role="listitem">
Azzuni, et al. 2025. <span>“Voice Cloning: A Comprehensive
Survey.”</span> <a
href="https://arxiv.org/pdf/2505.00579">https://arxiv.org/pdf/2505.00579</a>.
</div>
<div id="ref-babyvedat_vctk" class="csl-entry" role="listitem">
babyvedat. 2024. <span>“VCTK.”</span> Dataset on HuggingFace. <a
href="https://huggingface.co/datasets/badayvedat/VCTK">https://huggingface.co/datasets/badayvedat/VCTK</a>.
</div>
<div id="ref-kameoka2025latentvoicegrad" class="csl-entry"
role="listitem">
Kameoka, Hirokazu, Takuhiro Kaneko, Kou Tanaka, and Yuto Kondo. 2025.
<span>“LatentVoiceGrad: Nonparallel Voice Conversion with Latent
Diffusion/Flow-Matching Models.”</span> <a
href="https://arxiv.org/abs/2509.08379">https://arxiv.org/abs/2509.08379</a>.
</div>
<div id="ref-luo2025decodec" class="csl-entry" role="listitem">
Luo, Xiaoxue, Jinwei Huang, Runyan Yang, Yingying Gao, Junlan Feng, Chao
Deng, and Shilei Zhang. 2025. <span>“DeCodec: Rethinking Audio Codecs as
Universal Disentangled Representation Learners.”</span> <a
href="https://arxiv.org/pdf/2509.09201">https://arxiv.org/pdf/2509.09201</a>.
</div>
<div id="ref-quamer2025darkstream" class="csl-entry" role="listitem">
Quamer, et al. 2025. <span>“DarkStream: Real-Time Speech Anonymization
with Low Latency.”</span> <a
href="https://arxiv.org/pdf/2509.04667">https://arxiv.org/pdf/2509.04667</a>.
</div>
<div id="ref-yamagishi2019cstr" class="csl-entry" role="listitem">
Yamagishi, Junichi, Christophe Veaux, and Kirsten MacDonald. 2019.
<span>“CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice
Cloning Toolkit (Version 0.92).”</span> University of Edinburgh. The
Centre for Speech Technology Research (CSTR); <a
href="https://doi.org/10.7488/ds/2645"
class="uri">https://doi.org/10.7488/ds/2645</a>.
</div>
<div id="ref-yao2025pureformervc" class="csl-entry" role="listitem">
Yao, Wenhan, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, and Weiping
Wen. 2025. <span>“Pureformer-VC: Non-Parallel Voice Conversion with Pure
Stylized Transformer Blocks and Triplet Discriminative Training.”</span>
<a
href="https://arxiv.org/pdf/2506.08348">https://arxiv.org/pdf/2506.08348</a>.
</div>
</div>
</body>
</html>
