<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>voice_fake</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="./index.html">Home</a></p>
<p>Akshay Gulabrao 13 September 2025</p>
<h1 id="introduction-to-the-world-of-voice-synthesis">introduction to
the world of voice synthesis</h1>
<p><em>This is not a serious introduction to voice synthesis. This is my
attempt to become familiar with the field.</em></p>
<p>Voice synthesis refers to the deep learning task of converting one
voice speaking a specific utterance to a different voice. It is
significantly easier than face generation because the data has far lower
dimensionality. Azzuni et al. published a comprehensive survey of voice
cloning, so I will begin there. <a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Reading the introduction to they paper, the papers they cite mainly
involve TTS speech synthesis, however, I’m mainly interested in speech
to speech synthesis. An arxive search of speech synthesis yielded the
interesting paper by Quamer et al. <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> The author uses the
CommonVoice dataset.<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a></p>
<p>I also looked into the Pureformer VC <a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.
Let’s start with VCTK. A huggingface dataset already exists for this, so
it should be quite simple with the dataloader.</p>
<h2 id="blog">Blog</h2>
<p>9/13/2025: Didn’t get much done today. Wrote the intro and maybe a
paragraph. Downloaded common voice. Decided I’m going to start trying to
get a job in ML again (even though its probably hopeless). Commonvoice
is still getting parsed. Looking at tinygrad and trying to get a
codebase that works for me. Will probably involve using torch for the
auxillary utils and tinygrad for the core. Or maybe even start my own
library. In fact tomorrow, I’ll probably start my own library that just
imports tinygrad.</p>
<p>9/14/2025: Downloading the data is taking an absurdly long time. If
this finishes, my first task is to make downloading the data and
building the data loader significantly faster. I find it horrible that
the Open Voice dataset does nothing to advertise the length of the
dataset, and they only advertise the number of clips. It took over 30
minutes just to list all the clips. There are approximately 2.5 million
clips. You can use the first digit of the clip as the heuristic for how
close it is to being done, since it processes them in alphabetical
order. I really want to have this published on huggingface to make it
faster for everyone else. If I can do that, then these entire 2 days
won’t have been a waste. Someone in the future will be able to quickly
download and start training on this data. This was a bad idea. I should
have stuck to a proof of concept first. I don’t have the resources to
train it even if I could download it.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Azzuni et al. <a
href="https://arxiv.org/pdf/2505.00579">Voice Cloning: A Comprehensive
Survey.</a> 2025. arxiv.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Quamer et al. <a
href="https://arxiv.org/pdf/2509.04667">DarkStream: real-time speech
anonymization with low latency</a> 2025. arxiv.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Ardila, R., Branson, M., Davis, K., Henretty, M.,
Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M. and Weber,
G. (2020) <a href="https://arxiv.org/pdf/1912.06670">Common Voice: A
Massively-Multilingual Speech Corpus</a>. Proceedings of the 12th
Conference on Language Resources and Evaluation (LREC 2020).
pp. 4211—4215<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang
He, Weiping Wen. <a
href="https://arxiv.org/pdf/2506.08348">Pureformer-VC: Non-parallel
Voice Conversion with Pure Stylized Transformer Blocks and Triplet
Discriminative Training</a>. IJCNN2025.<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
