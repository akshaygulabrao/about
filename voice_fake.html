<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>voice_fake</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
<p><a href="./index.html">Home</a></p>
<p>Akshay Gulabrao 13 September 2025</p>
<h1 id="introduction-to-the-world-of-voice-synthesis">introduction to
the world of voice synthesis</h1>
<p><em>This is not a serious introduction to voice synthesis. This is my
attempt to become familiar with the field.</em></p>
<p>Voice synthesis refers to the deep learning task of converting one
voice speaking a specific utterance to a different voice. It is
significantly easier than face generation because the data has far lower
dimensionality. Azzuni et al. published a comprehensive survey of voice
cloning, so I will begin there. <a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Reading the introduction to they paper, the papers they cite mainly
involve TTS speech synthesis, however, I’m mainly interested in speech
to speech synthesis. An arxive search of speech synthesis yielded the
interesting paper by Quamer et al. <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> The author uses the
CommonVoice dataset.<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a></p>
<p>Another interesting paper that I found by Luo et al. innovates on a
codec framework with a subspace orthogonal projection module that splits
the input into 2 subspaces where 1 correlates to speech and another
correlates to the background.</p>
<h2 id="blog">Blog</h2>
<p><strong>September 14, 2025</strong> Downloading and indexing the Open
Voice dataset was extremely slow—listing all ~2.5 million clips took
over 30 minutes. Since clips are processed alphabetically, I used the
first digit of the clip name as a rough progress indicator. The dataset
only advertises clip count, not size or download time, which makes
planning difficult. I ended up switching to a smaller dataset due to
resource constraints. I plan to publish a preprocessed version on
Hugging Face to make it easier for others to use.</p>
<p><strong>September 13, 2025</strong> Downloaded Common Voice and
started drafting the intro. Parsing is still in progress. I’ve been
exploring Tinygrad as a core framework, possibly using PyTorch for
utilities. I might build a small wrapper library around Tinygrad to
streamline experimentation.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Azzuni et al. <a
href="https://arxiv.org/pdf/2505.00579">Voice Cloning: A Comprehensive
Survey.</a> 2025. arxiv.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Quamer et al. <a
href="https://arxiv.org/pdf/2509.04667">DarkStream: real-time speech
anonymization with low latency</a> 2025. arxiv.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Ardila, R., Branson, M., Davis, K., Henretty, M.,
Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M. and Weber,
G. (2020) <a href="https://arxiv.org/pdf/1912.06670">Common Voice: A
Massively-Multilingual Speech Corpus</a>. Proceedings of the 12th
Conference on Language Resources and Evaluation (LREC 2020).
pp. 4211—4215<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
